{"version":3,"file":"static/chunks/1682.ffa6dfdc0ec8552d.js","mappings":"mLAAO,cACP,2CACA,oCACA,gCAEA,mBACA,CACA,sBACA,CAaO,cACP,MAGA,WACA,oCACA,8CAEA,EACA,sBAEA,OADA,qBACA,CACA,CACA,CACA,CAeO,gBACP,SACA,cACA,aACA,GAEA,qBACA,gDACA,uDACK,EACL,cACA,8BACA,YACA,UACA,QAEA,KApBO,KAqBP,IApBA,8CACA,+CAoBA,4BACA,iBAIA,UACA,IAEA,EAEA,CAAK,EACL,EACA,wBChFe,SACf,iBACA,qBACA,aAD4C,IAC5C,EACA,CACA,WACA,CAH0C,KAG1C,GAAkB,mBAAmB,GAAG,kBAAkB,EAE1D,aACA,4EAEA,CACO,uBACP,KACA,+DAEA,kCACA,mBACA,iBACA,aACA,WACA,sBACA,CCrBe,QACf,qBACA,YACA,YACA,WACA,mBACA,CACA,iBACA,SAAkB,UAAU,IAAI,WAAW,OAAO,SAAS,gBAAgB,mBAAmB,GAE9F,WACA,4BACA,CACA,aACA,oCACA,6BACA,eAEA,qBACA,2BACA,kBAEA,sDAEA,CCzBe,QACf,wBAAkB,uBAAuC,EACzD,kBACA,mBACA,CACA,sBAA+B,EAC/B,IAAgB,gBAA6B,oBAC7C,QACA,CACA,2BACA,EACA,iBACA,EACA,EAGA,CAEA,CACA,gBAAyB,EAOzB,OANA,aACA,sCAEA,MADA,mBACA,CACA,EAAa,EAEb,YAEA,sBAAoC,EACpC,MAEA,mBADA,sBACA,0CACA,CACA,CCTe,gBAAyB,EACxC,OADiD,SACjD,MAAsC,EACtC,0BACA,mBACA,eAIA,CADA,aAFA,UAMA,UAAgB,GAAQ,oBACxB,EACA,YAEA,EACA,CAEA,iBAA0B,MAsC1B,EArCA,wCACA,QAA4B,WAAK,IAGjC,GAFQ,EAAgB,UAExB,IAFwB,MAvCI,EAyC5B,kBACA,8BAIA,uBACA,mBAOA,EALA,CACA,YACA,QACA,OACA,CACA,OACA,MACA,iDAAiE,EAAY,GAE7E,OACA,sBACA,wBACA,qBACA,EACA,oBAIA,gCACA,oBAEA,oBACA,aAAgB,iBAA2B,uCAE3C,OA+CA,OACA,QA9CA,8BAKA,EAHA,uBACA,KACA,SAEA,YAA4B,IAAc,MAC1C,wBAEA,GADA,KACA,QACA,0EAEA,cACA,uBACA,KACA,OACA,6BAEA,OACA,KACA,CACA,uBACA,KACA,eACA,YAAoC,IAAgB,MACpD,MAAkC,EAAS,KAC3C,EAD2C,EACA,UAC3C,GACA,2BACA,SAAwC,EAAK,MAC7C,CACA,MACA,CACA,CAEA,uBACA,KACA,eACA,YAA4B,IAAiB,KAC7C,KAAiC,EAAS,KAC1C,EAD0C,CAC1C,EACA,8BAEA,gBAAqB,wBACrB,CAAS,EAGT,WACA,aA1DA,MA2DA,aA1DA,WA2DA,YACA,gBACA,gBACA,eAhFA,gDAiFA,SACA,cACA,cACA,kBACA,CACA,CACA,oBAEA,OAAiB,UADS,EAAa,UAAD,KAAiB,wBACtC,CACjB,CACA,mBACA,QACA,IACA,KACA,KACA,YAAwB,WAAuB,KAC/C,UACA,QACA,6BACA,uBACA,OACA,MACA,CACA,MACA,IACA,CAEA,mBAAiB,gBACjB,CACA,+BAAqD,MAxJrD,IAyJA,KACA,MAEA,0BACA,mBACA,cACA,SAEA,mBACA,MACA,SAEA,sBACA,iBA3KA,IA2KA,qBACA,uBACA,GA7KA,GA6KA,CACA,IAAkB,EAAa,OAE/B,IAF+B,IAE/B,iDAGA,MA3KA,CACA,MACA,IAJA,GA6KA,EA7KA,IAAc,EAId,KAHA,GA4KA,EA5KA,IA4KoD,EAzKpD,GACA,sBACA,wBACA,0BACA,4BACA,CAqKA,KAEA,iBACA,YAAkC,KAAY,IAC9C,iBACA,2BACA,WAAwC,EAAK,kBAO7C,2BACA,OACA,sBACA,sBACA,YAA6B,KAAa,KAC1C,uBACA,GACA,wBACA,KAGA,CACA,OAAe,EAAc,IAC7B,CACA,CC3MA,MDyM6B,GCzM7B,OACA,yBACA,CACe,gBAAkB,EACjC,OAD0C,KAC1C,GACA,SACA,oBACA,aACA,eACA,CACA,sBAAsC,EACtC,0BACA,mBACA,eAIA,CADA,aAFA,UAMA,IAAgB,SAAQ,oBACxB,EACA,YAEA,EACA,CACA,WACA,kDACA,CACA,kBACA,uBAEA,GAAyB,4BAAkC,OAC3D,MACA,iDAAiE,EAAY,GAE7E,OACA,uBACA,yBACA,uBACA,EACA,sBACA,gCACA,sBACA,sBACA,aAAgB,iBAA2B,2CAC3C,OACA,cACA,cACA,YACA,WACA,gBACA,SACA,eAtBA,+CAuBA,CACA,CACA,mBACA,QACA,IACA,KACA,KACA,YAAwB,WAAuB,KAC/C,UACA,QACA,6BACA,uBACA,OACA,MACA,CACA,MACA,IACA,CAEA,mBAAiB,gBACjB,CAEA,iBAA0B,MAG1B,EA4BA,EA9BA,YAA4B,WAAK,oCAGjC,aArF6B,EAqF7B,kBACA,SAEA,aAvF6B,EAuF7B,kBACA,SAGA,6BAGA,gCACA,4BACA,8CACA,sCACA,oBACA,WACA,wBACA,CACA,eACA,cAA+B,CAC/B,cACA,eAAiC,oBAA0B,CAC3D,sCACA,gBACA,EACA,sBAGA,SACA,gCAKA,EAHA,KAGuB,CAHvB,iBACA,KACA,SAEA,YAA4B,IAAc,MAC1C,wBACA,uBAGA,6BACA,UAEA,CACA,MAAoC,EAAS,SAC7C,yBACA,0BACA,MACA,eACA,YAAoC,IAAgB,MACpD,MAAkC,EAAS,KAC3C,EAD2C,EACA,UAC3C,GAEA,SAAwC,EAAK,MAC7C,CACA,MACA,CACA,CACA,gBAAqB,UACrB,CAAS,EACT,OACA,KACA,OACA,WACA,mBACA,gBACA,aACA,UACA,iBACA,+BACA,cACA,CACA,CACA,oBAEA,OAAiB,UADS,EAAa,UAAD,KAAiB,wBACtC,CACjB,CACA,+BAAqD,EACrD,KACA,MAEA,0BACA,mBACA,cACA,SAEA,mBACA,MACA,SAGA,yBACA,GADyD,CACzD,CAEA,iBACA,YAAkC,KAAY,IAC9C,iBACA,2BACA,WAAwC,EAAK,kBAK7C,OAAe,EAAc,MAAa,EAAa,IAA1B,CAC7B,CAIA,KALuD,IAKvD,SAhMA,EAiMA,OACA,CADkB,EAElB,MAEA,mBACA,gBAEA,KACA,QACA,IACA,6BACA,KACA,KAAe,cAAiB,SA7MhC,EA6MgC,MA5MhC,MA4MgC,MAChC,eACA,WACA,kCACA,qBAAyC,EAAI,GAAG,GAAK,iDAAiD,cAAc,UAAU,WAAW,2DAEzI,aACA,CACA,QACA,CACA,CCnNA,oEACe,SA2Bf,kBAAkB,yIAA8I,EAChK,KACA,uBAEA,KACA,oBAAkC,WAAS,SAE3C,KACA,oBAAkC,YAAU,SAG5C,0DAEA,KACA,eAA6B,EAAG,CAChC,OADgC,IAChC,EACA,eACA,CAAa,OAEb,KACA,eAA6B,EAAG,YAChC,EACA,eACA,CAAa,OAEb,KACA,eAA6B,EAAG,CAChC,OADgC,IAChC,IAAgC,WAAS,IACzC,eACA,CAAa,OAEb,KACA,eAA6B,EAAG,YAChC,IAAgC,WAAS,IACzC,eACA,CAAa,OAEb,KACA,eAA6B,EAAG,CAChC,OADgC,IAChC,IAAgC,WAAS,IAAI,EAAK,OAClD,eACA,CAAa,OAEb,KACA,eAA6B,EAAG,YAChC,IAAgC,YAAU,GAC1C,CAAa,OAEb,KACA,eAA6B,EAAG,CAChC,OADgC,IAChC,IAAgC,YAAU,GAC1C,CAAa,OAEb,KACA,eAA6B,EAAG,CAChC,OADgC,IAChC,IAAgC,YAAU,IAAI,EAAI,MAClD,CAAa,OAGb,qGAEA,qBACA,oBAA8B,GAAqB,EACnD,SAAuB,MAAG,CAAG,4BAAiD,EAC9E,qCAA2D,EAAQ,CACnE,CAAS,CACT,CAcA,4BACA,QACA,EAEA,EADA,QAEA,sBACA,KAGA,IACA,iBACA,YAEA,sCACQ,EAAgB,GACxB,WADwB,CACxB,MACA,2BACA,WACA,8FAEA,SACA,OAEA,+CAGA,aAFQ,EAAgB,GAExB,IACA,OAHwB,IAGJ,6BAAiC,4CACzC,EAAgB,GAC5B,QACA,GAF4B,CAG5B,mEAIA,gBAjJA,0BAiJA,GACA,sBACA,EACA,EACA,MAEA,QADA,sBAEA,MAEA,cACA,KACA,CAEA,QADA,sBAEA,MAEA,mBACA,kEACA,CAEA,MACA,qCACA,GACA,CAEA,oBAAwB,cAA4B,0BACpD,KACA,IAYA,SACA,SACA,oBACA,QAEA,oBAIA,OAEA,KACA,CACA,CACA,CACA,sBAA+B,EAC/B,gCACA,CAKA,0BAAmC,EACnC,IAAgB,2CAAwC,0BAChD,EAAgB,UACxB,IADwB,EACxB,wCAGA,gCACA,QAA4B,WAAK,IAEjC,MAEA,SAEA,kBACA,YAA4B,YAC5B,oBAD8C,KAF9C,KAMA,MACA,MAGA,wBACA,CACA,QACA,CAOA,oBAA6B,EAE7B,MADA,gCACA,gBACA,CAKA,oCAA6C,EAE7C,OADA,2BACA,YAiBA,qBACA,kBAAgB,wCAAkD,EAElE,sBACA,OAAqB,aAGrB,QAAc,iBAAkB,EAChC,GACA,MAEA,GACA,MAEA,GACA,MAEA,WACA,MAEA,sBAIA,IACA,CADqC,CACrC,EACA,KACA,OACA,WACA,YAAwB,MAAW,IACnC,sBACA,SACA,sCACA,EACA,OACA,WACA,CACA,MAEA,SAMA,IALA,4BAEA,sBACA,OAEA,MAMA,gBAEA,OAPA,OACA,kBACA,WACA,CACA,MAWA,oBACA,oBAEA,UAEA,WACA,kCACA,mCACA,EACA,OACA,WACA,EAKA,GAFA,MACA,OACA,EACA,KAEA,CAEA,OACA,kBACA,WACA,CACA,CACA,kBACA,iBAMA,2BACA,iBAcA,MACA,UACA,KAhBA,CACA,QAA6B,EAC7B,YAA4B,WAAiB,MAC7C,KAAmC,IAAnC,GAAmC,yBACnC,kBAAkD,IAClD,SACA,aAEA,8BACA,KACA,CACA,OAEA,CAIA,QACA,CASA,sBAAsC,EACtC,gCACA,CACA,0BAA0C,EAC1C,MAAkB,QAAM,UACxB,WAAgB,YAAoB,sCACpC,sBACA,CAKA,sBAAgC,EAGhC,qEACA,MAAe,qBAAe,KAC9B,CACA","sources":["webpack://_N_E/./node_modules/@gmod/tabix/esm/util.js","webpack://_N_E/./node_modules/@gmod/tabix/esm/virtualOffset.js","webpack://_N_E/./node_modules/@gmod/tabix/esm/chunk.js","webpack://_N_E/./node_modules/@gmod/tabix/esm/indexFile.js","webpack://_N_E/./node_modules/@gmod/tabix/esm/tbi.js","webpack://_N_E/./node_modules/@gmod/tabix/esm/csi.js","webpack://_N_E/./node_modules/@gmod/tabix/esm/tabixIndexedFile.js","webpack://_N_E/./node_modules/@gmod/tabix/esm/index.js"],"sourcesContent":["export function longToNumber(long) {\n    if (long.greaterThan(Number.MAX_SAFE_INTEGER) ||\n        long.lessThan(Number.MIN_SAFE_INTEGER)) {\n        throw new Error('integer overflow');\n    }\n    return long.toNumber();\n}\nclass AbortError extends Error {\n}\n/**\n * Properly check if the given AbortSignal is aborted. Per the standard, if the\n * signal reads as aborted, this function throws either a DOMException\n * AbortError, or a regular error with a `code` attribute set to `ERR_ABORTED`.\n *\n * For convenience, passing `undefined` is a no-op\n *\n * @param {AbortSignal} [signal] an AbortSignal, or anything with an `aborted`\n * attribute\n *\n * @returns nothing\n */\nexport function checkAbortSignal(signal) {\n    if (!signal) {\n        return;\n    }\n    if (signal.aborted) {\n        if (typeof DOMException !== 'undefined') {\n            throw new DOMException('aborted', 'AbortError');\n        }\n        else {\n            const e = new AbortError('aborted');\n            e.code = 'ERR_ABORTED';\n            throw e;\n        }\n    }\n}\n/**\n * Skips to the next tick, then runs `checkAbortSignal`.\n * Await this to inside an otherwise synchronous loop to\n * provide a place to break when an abort signal is received.\n * @param {AbortSignal} signal\n */\nexport async function abortBreakPoint(signal) {\n    await Promise.resolve();\n    checkAbortSignal(signal);\n}\nexport function canMergeBlocks(chunk1, chunk2) {\n    return (chunk2.minv.blockPosition - chunk1.maxv.blockPosition < 65000 &&\n        chunk2.maxv.blockPosition - chunk1.minv.blockPosition < 5000000);\n}\nexport function optimizeChunks(chunks, lowest) {\n    const mergedChunks = [];\n    let lastChunk = null;\n    if (chunks.length === 0) {\n        return chunks;\n    }\n    chunks.sort(function (c0, c1) {\n        const dif = c0.minv.blockPosition - c1.minv.blockPosition;\n        return dif !== 0 ? dif : c0.minv.dataPosition - c1.minv.dataPosition;\n    });\n    chunks.forEach(chunk => {\n        if (!lowest || chunk.maxv.compareTo(lowest) > 0) {\n            if (lastChunk === null) {\n                mergedChunks.push(chunk);\n                lastChunk = chunk;\n            }\n            else {\n                if (canMergeBlocks(lastChunk, chunk)) {\n                    if (chunk.maxv.compareTo(lastChunk.maxv) > 0) {\n                        lastChunk.maxv = chunk.maxv;\n                    }\n                }\n                else {\n                    mergedChunks.push(chunk);\n                    lastChunk = chunk;\n                }\n            }\n        }\n    });\n    return mergedChunks;\n}\n//# sourceMappingURL=util.js.map","export default class VirtualOffset {\n    constructor(blockPosition, dataPosition) {\n        this.blockPosition = blockPosition; // < offset of the compressed data block\n        this.dataPosition = dataPosition; // < offset into the uncompressed data\n    }\n    toString() {\n        return `${this.blockPosition}:${this.dataPosition}`;\n    }\n    compareTo(b) {\n        return (this.blockPosition - b.blockPosition || this.dataPosition - b.dataPosition);\n    }\n}\nexport function fromBytes(bytes, offset = 0, bigendian = false) {\n    if (bigendian) {\n        throw new Error('big-endian virtual file offsets not implemented');\n    }\n    return new VirtualOffset(bytes[offset + 7] * 0x10000000000 +\n        bytes[offset + 6] * 0x100000000 +\n        bytes[offset + 5] * 0x1000000 +\n        bytes[offset + 4] * 0x10000 +\n        bytes[offset + 3] * 0x100 +\n        bytes[offset + 2], (bytes[offset + 1] << 8) | bytes[offset]);\n}\n//# sourceMappingURL=virtualOffset.js.map","// little class representing a chunk in the index\nexport default class Chunk {\n    constructor(minv, maxv, bin, fetchedSize = undefined) {\n        this.minv = minv;\n        this.maxv = maxv;\n        this.bin = bin;\n        this._fetchedSize = fetchedSize;\n    }\n    toUniqueString() {\n        return `${this.minv}..${this.maxv} (bin ${this.bin}, fetchedSize ${this.fetchedSize()})`;\n    }\n    toString() {\n        return this.toUniqueString();\n    }\n    compareTo(b) {\n        return (this.minv.compareTo(b.minv) ||\n            this.maxv.compareTo(b.maxv) ||\n            this.bin - b.bin);\n    }\n    fetchedSize() {\n        if (this._fetchedSize !== undefined) {\n            return this._fetchedSize;\n        }\n        return this.maxv.blockPosition + (1 << 16) - this.minv.blockPosition;\n    }\n}\n//# sourceMappingURL=chunk.js.map","export default class IndexFile {\n    constructor({ filehandle, renameRefSeqs = (n) => n, }) {\n        this.filehandle = filehandle;\n        this.renameRefSeq = renameRefSeqs;\n    }\n    async getMetadata(opts = {}) {\n        const { indices: _indices, ...rest } = await this.parse(opts);\n        return rest;\n    }\n    _findFirstData(currentFdl, virtualOffset) {\n        if (currentFdl) {\n            return currentFdl.compareTo(virtualOffset) > 0\n                ? virtualOffset\n                : currentFdl;\n        }\n        else {\n            return virtualOffset;\n        }\n    }\n    async parse(opts = {}) {\n        if (!this.parseP) {\n            this.parseP = this._parse(opts).catch((e) => {\n                this.parseP = undefined;\n                throw e;\n            });\n        }\n        return this.parseP;\n    }\n    async hasRefSeq(seqId, opts = {}) {\n        var _a;\n        const idx = await this.parse(opts);\n        return !!((_a = idx.indices[seqId]) === null || _a === void 0 ? void 0 : _a.binIndex);\n    }\n}\n//# sourceMappingURL=indexFile.js.map","import Long from 'long';\nimport VirtualOffset, { fromBytes } from './virtualOffset';\nimport Chunk from './chunk';\nimport { unzip } from '@gmod/bgzf-filehandle';\nimport { longToNumber, optimizeChunks, checkAbortSignal } from './util';\nimport IndexFile from './indexFile';\nconst TBI_MAGIC = 21578324; // TBI\\1\nconst TAD_LIDX_SHIFT = 14;\n/**\n * calculate the list of bins that may overlap with region [beg,end)\n * (zero-based half-open)\n */\nfunction reg2bins(beg, end) {\n    beg += 1; // < convert to 1-based closed\n    end -= 1;\n    return [\n        [0, 0],\n        [1 + (beg >> 26), 1 + (end >> 26)],\n        [9 + (beg >> 23), 9 + (end >> 23)],\n        [73 + (beg >> 20), 73 + (end >> 20)],\n        [585 + (beg >> 17), 585 + (end >> 17)],\n        [4681 + (beg >> 14), 4681 + (end >> 14)],\n    ];\n}\nexport default class TabixIndex extends IndexFile {\n    async lineCount(refName, opts = {}) {\n        const indexData = await this.parse(opts);\n        const refId = indexData.refNameToId[refName];\n        if (refId === undefined) {\n            return -1;\n        }\n        const idx = indexData.indices[refId];\n        if (!idx) {\n            return -1;\n        }\n        const { stats } = indexData.indices[refId];\n        if (stats) {\n            return stats.lineCount;\n        }\n        return -1;\n    }\n    // fetch and parse the index\n    async _parse(opts = {}) {\n        const buf = await this.filehandle.readFile(opts);\n        const bytes = await unzip(buf);\n        checkAbortSignal(opts.signal);\n        // check TBI magic numbers\n        if (bytes.readUInt32LE(0) !== TBI_MAGIC /* \"TBI\\1\" */) {\n            throw new Error('Not a TBI file');\n            // TODO: do we need to support big-endian TBI files?\n        }\n        // number of reference sequences in the index\n        const refCount = bytes.readInt32LE(4);\n        const formatFlags = bytes.readInt32LE(8);\n        const coordinateType = formatFlags & 0x10000 ? 'zero-based-half-open' : '1-based-closed';\n        const formatOpts = {\n            0: 'generic',\n            1: 'SAM',\n            2: 'VCF',\n        };\n        const format = formatOpts[formatFlags & 0xf];\n        if (!format) {\n            throw new Error(`invalid Tabix preset format flags ${formatFlags}`);\n        }\n        const columnNumbers = {\n            ref: bytes.readInt32LE(12),\n            start: bytes.readInt32LE(16),\n            end: bytes.readInt32LE(20),\n        };\n        const metaValue = bytes.readInt32LE(24);\n        const depth = 5;\n        const maxBinNumber = ((1 << ((depth + 1) * 3)) - 1) / 7;\n        const maxRefLength = 2 ** (14 + depth * 3);\n        const metaChar = metaValue ? String.fromCharCode(metaValue) : null;\n        const skipLines = bytes.readInt32LE(28);\n        // read sequence dictionary\n        const nameSectionLength = bytes.readInt32LE(32);\n        const { refNameToId, refIdToName } = this._parseNameBytes(bytes.slice(36, 36 + nameSectionLength));\n        // read the indexes for each reference sequence\n        let currOffset = 36 + nameSectionLength;\n        let firstDataLine;\n        const indices = new Array(refCount).fill(0).map(() => {\n            // the binning index\n            const binCount = bytes.readInt32LE(currOffset);\n            currOffset += 4;\n            const binIndex = {};\n            let stats;\n            for (let j = 0; j < binCount; j += 1) {\n                const bin = bytes.readUInt32LE(currOffset);\n                currOffset += 4;\n                if (bin > maxBinNumber + 1) {\n                    throw new Error('tabix index contains too many bins, please use a CSI index');\n                }\n                else if (bin === maxBinNumber + 1) {\n                    const chunkCount = bytes.readInt32LE(currOffset);\n                    currOffset += 4;\n                    if (chunkCount === 2) {\n                        stats = this.parsePseudoBin(bytes, currOffset);\n                    }\n                    currOffset += 16 * chunkCount;\n                }\n                else {\n                    const chunkCount = bytes.readInt32LE(currOffset);\n                    currOffset += 4;\n                    const chunks = new Array(chunkCount);\n                    for (let k = 0; k < chunkCount; k += 1) {\n                        const u = fromBytes(bytes, currOffset);\n                        const v = fromBytes(bytes, currOffset + 8);\n                        currOffset += 16;\n                        firstDataLine = this._findFirstData(firstDataLine, u);\n                        chunks[k] = new Chunk(u, v, bin);\n                    }\n                    binIndex[bin] = chunks;\n                }\n            }\n            // the linear index\n            const linearCount = bytes.readInt32LE(currOffset);\n            currOffset += 4;\n            const linearIndex = new Array(linearCount);\n            for (let k = 0; k < linearCount; k += 1) {\n                linearIndex[k] = fromBytes(bytes, currOffset);\n                currOffset += 8;\n                firstDataLine = this._findFirstData(firstDataLine, linearIndex[k]);\n            }\n            return { binIndex, linearIndex, stats };\n        });\n        return {\n            indices,\n            metaChar,\n            maxBinNumber,\n            maxRefLength,\n            skipLines,\n            firstDataLine,\n            columnNumbers,\n            coordinateType,\n            format,\n            refIdToName,\n            refNameToId,\n            maxBlockSize: 1 << 16,\n        };\n    }\n    parsePseudoBin(bytes, offset) {\n        const lineCount = longToNumber(Long.fromBytesLE(bytes.slice(offset + 16, offset + 24), true));\n        return { lineCount };\n    }\n    _parseNameBytes(namesBytes) {\n        let currRefId = 0;\n        let currNameStart = 0;\n        const refIdToName = [];\n        const refNameToId = {};\n        for (let i = 0; i < namesBytes.length; i += 1) {\n            if (!namesBytes[i]) {\n                if (currNameStart < i) {\n                    let refName = namesBytes.toString('utf8', currNameStart, i);\n                    refName = this.renameRefSeq(refName);\n                    refIdToName[currRefId] = refName;\n                    refNameToId[refName] = currRefId;\n                }\n                currNameStart = i + 1;\n                currRefId += 1;\n            }\n        }\n        return { refNameToId, refIdToName };\n    }\n    async blocksForRange(refName, min, max, opts = {}) {\n        if (min < 0) {\n            min = 0;\n        }\n        const indexData = await this.parse(opts);\n        const refId = indexData.refNameToId[refName];\n        if (refId === undefined) {\n            return [];\n        }\n        const ba = indexData.indices[refId];\n        if (!ba) {\n            return [];\n        }\n        const minOffset = ba.linearIndex.length\n            ? ba.linearIndex[min >> TAD_LIDX_SHIFT >= ba.linearIndex.length\n                ? ba.linearIndex.length - 1\n                : min >> TAD_LIDX_SHIFT]\n            : new VirtualOffset(0, 0);\n        if (!minOffset) {\n            console.warn('querying outside of possible tabix range');\n        }\n        // const { linearIndex, binIndex } = indexes\n        const overlappingBins = reg2bins(min, max); // List of bin #s that overlap min, max\n        const chunks = [];\n        // Find chunks in overlapping bins.  Leaf bins (< 4681) are not pruned\n        for (const [start, end] of overlappingBins) {\n            for (let bin = start; bin <= end; bin++) {\n                if (ba.binIndex[bin]) {\n                    for (const c of ba.binIndex[bin]) {\n                        chunks.push(new Chunk(c.minv, c.maxv, bin));\n                    }\n                }\n            }\n        }\n        // Use the linear index to find minimum file position of chunks that could\n        // contain alignments in the region\n        const nintv = ba.linearIndex.length;\n        let lowest = null;\n        const minLin = Math.min(min >> 14, nintv - 1);\n        const maxLin = Math.min(max >> 14, nintv - 1);\n        for (let i = minLin; i <= maxLin; ++i) {\n            const vp = ba.linearIndex[i];\n            if (vp) {\n                if (!lowest || vp.compareTo(lowest) < 0) {\n                    lowest = vp;\n                }\n            }\n        }\n        return optimizeChunks(chunks, lowest);\n    }\n}\n//# sourceMappingURL=tbi.js.map","import Long from 'long';\nimport { unzip } from '@gmod/bgzf-filehandle';\nimport VirtualOffset, { fromBytes } from './virtualOffset';\nimport Chunk from './chunk';\nimport { longToNumber, optimizeChunks } from './util';\nimport IndexFile from './indexFile';\nconst CSI1_MAGIC = 21582659; // CSI\\1\nconst CSI2_MAGIC = 38359875; // CSI\\2\nfunction lshift(num, bits) {\n    return num * 2 ** bits;\n}\nfunction rshift(num, bits) {\n    return Math.floor(num / 2 ** bits);\n}\nexport default class CSI extends IndexFile {\n    constructor(args) {\n        super(args);\n        this.maxBinNumber = 0;\n        this.depth = 0;\n        this.minShift = 0;\n    }\n    async lineCount(refName, opts = {}) {\n        const indexData = await this.parse(opts);\n        const refId = indexData.refNameToId[refName];\n        if (refId === undefined) {\n            return -1;\n        }\n        const idx = indexData.indices[refId];\n        if (!idx) {\n            return -1;\n        }\n        const { stats } = indexData.indices[refId];\n        if (stats) {\n            return stats.lineCount;\n        }\n        return -1;\n    }\n    indexCov() {\n        throw new Error('CSI indexes do not support indexcov');\n    }\n    parseAuxData(bytes, offset) {\n        const formatFlags = bytes.readInt32LE(offset);\n        const coordinateType = formatFlags & 0x10000 ? 'zero-based-half-open' : '1-based-closed';\n        const format = { 0: 'generic', 1: 'SAM', 2: 'VCF' }[formatFlags & 0xf];\n        if (!format) {\n            throw new Error(`invalid Tabix preset format flags ${formatFlags}`);\n        }\n        const columnNumbers = {\n            ref: bytes.readInt32LE(offset + 4),\n            start: bytes.readInt32LE(offset + 8),\n            end: bytes.readInt32LE(offset + 12),\n        };\n        const metaValue = bytes.readInt32LE(offset + 16);\n        const metaChar = metaValue ? String.fromCharCode(metaValue) : null;\n        const skipLines = bytes.readInt32LE(offset + 20);\n        const nameSectionLength = bytes.readInt32LE(offset + 24);\n        const { refIdToName, refNameToId } = this._parseNameBytes(bytes.slice(offset + 28, offset + 28 + nameSectionLength));\n        return {\n            refIdToName,\n            refNameToId,\n            skipLines,\n            metaChar,\n            columnNumbers,\n            format,\n            coordinateType,\n        };\n    }\n    _parseNameBytes(namesBytes) {\n        let currRefId = 0;\n        let currNameStart = 0;\n        const refIdToName = [];\n        const refNameToId = {};\n        for (let i = 0; i < namesBytes.length; i += 1) {\n            if (!namesBytes[i]) {\n                if (currNameStart < i) {\n                    let refName = namesBytes.toString('utf8', currNameStart, i);\n                    refName = this.renameRefSeq(refName);\n                    refIdToName[currRefId] = refName;\n                    refNameToId[refName] = currRefId;\n                }\n                currNameStart = i + 1;\n                currRefId += 1;\n            }\n        }\n        return { refNameToId, refIdToName };\n    }\n    // fetch and parse the index\n    async _parse(opts = {}) {\n        const bytes = await unzip(await this.filehandle.readFile(opts));\n        // check TBI magic numbers\n        let csiVersion;\n        if (bytes.readUInt32LE(0) === CSI1_MAGIC) {\n            csiVersion = 1;\n        }\n        else if (bytes.readUInt32LE(0) === CSI2_MAGIC) {\n            csiVersion = 2;\n        }\n        else {\n            throw new Error('Not a CSI file');\n            // TODO: do we need to support big-endian CSI files?\n        }\n        this.minShift = bytes.readInt32LE(4);\n        this.depth = bytes.readInt32LE(8);\n        this.maxBinNumber = ((1 << ((this.depth + 1) * 3)) - 1) / 7;\n        const maxRefLength = 2 ** (this.minShift + this.depth * 3);\n        const auxLength = bytes.readInt32LE(12);\n        const aux = auxLength && auxLength >= 30\n            ? this.parseAuxData(bytes, 16)\n            : {\n                refIdToName: [],\n                refNameToId: {},\n                metaChar: null,\n                columnNumbers: { ref: 0, start: 1, end: 2 },\n                coordinateType: 'zero-based-half-open',\n                format: 'generic',\n            };\n        const refCount = bytes.readInt32LE(16 + auxLength);\n        // read the indexes for each reference sequence\n        let firstDataLine;\n        let currOffset = 16 + auxLength + 4;\n        const indices = new Array(refCount).fill(0).map(() => {\n            // the binning index\n            const binCount = bytes.readInt32LE(currOffset);\n            currOffset += 4;\n            const binIndex = {};\n            let stats; // < provided by parsing a pseudo-bin, if present\n            for (let j = 0; j < binCount; j += 1) {\n                const bin = bytes.readUInt32LE(currOffset);\n                if (bin > this.maxBinNumber) {\n                    // this is a fake bin that actually has stats information\n                    // about the reference sequence in it\n                    stats = this.parsePseudoBin(bytes, currOffset + 4);\n                    currOffset += 4 + 8 + 4 + 16 + 16;\n                }\n                else {\n                    const loffset = fromBytes(bytes, currOffset + 4);\n                    firstDataLine = this._findFirstData(firstDataLine, loffset);\n                    const chunkCount = bytes.readInt32LE(currOffset + 12);\n                    currOffset += 16;\n                    const chunks = new Array(chunkCount);\n                    for (let k = 0; k < chunkCount; k += 1) {\n                        const u = fromBytes(bytes, currOffset);\n                        const v = fromBytes(bytes, currOffset + 8);\n                        currOffset += 16;\n                        // this._findFirstData(data, u)\n                        chunks[k] = new Chunk(u, v, bin);\n                    }\n                    binIndex[bin] = chunks;\n                }\n            }\n            return { binIndex, stats };\n        });\n        return {\n            ...aux,\n            csi: true,\n            refCount,\n            maxBlockSize: 1 << 16,\n            firstDataLine,\n            csiVersion,\n            indices,\n            depth: this.depth,\n            maxBinNumber: this.maxBinNumber,\n            maxRefLength,\n        };\n    }\n    parsePseudoBin(bytes, offset) {\n        const lineCount = longToNumber(Long.fromBytesLE(bytes.slice(offset + 28, offset + 36), true));\n        return { lineCount };\n    }\n    async blocksForRange(refName, min, max, opts = {}) {\n        if (min < 0) {\n            min = 0;\n        }\n        const indexData = await this.parse(opts);\n        const refId = indexData.refNameToId[refName];\n        if (refId === undefined) {\n            return [];\n        }\n        const ba = indexData.indices[refId];\n        if (!ba) {\n            return [];\n        }\n        // const { linearIndex, binIndex } = indexes\n        const overlappingBins = this.reg2bins(min, max); // List of bin #s that overlap min, max\n        const chunks = [];\n        // Find chunks in overlapping bins.  Leaf bins (< 4681) are not pruned\n        for (const [start, end] of overlappingBins) {\n            for (let bin = start; bin <= end; bin++) {\n                if (ba.binIndex[bin]) {\n                    for (const c of ba.binIndex[bin]) {\n                        chunks.push(new Chunk(c.minv, c.maxv, bin));\n                    }\n                }\n            }\n        }\n        return optimizeChunks(chunks, new VirtualOffset(0, 0));\n    }\n    /**\n     * calculate the list of bins that may overlap with region [beg,end) (zero-based half-open)\n     */\n    reg2bins(beg, end) {\n        beg -= 1; // < convert to 1-based closed\n        if (beg < 1) {\n            beg = 1;\n        }\n        if (end > 2 ** 50) {\n            end = 2 ** 34;\n        } // 17 GiB ought to be enough for anybody\n        end -= 1;\n        let l = 0;\n        let t = 0;\n        let s = this.minShift + this.depth * 3;\n        const bins = [];\n        for (; l <= this.depth; s -= 3, t += lshift(1, l * 3), l += 1) {\n            const b = t + rshift(beg, s);\n            const e = t + rshift(end, s);\n            if (e - b + bins.length > this.maxBinNumber) {\n                throw new Error(`query ${beg}-${end} is too large for current binning scheme (shift ${this.minShift}, depth ${this.depth}), try a smaller query or a coarser index binning scheme`);\n            }\n            bins.push([b, e]);\n        }\n        return bins;\n    }\n}\n//# sourceMappingURL=csi.js.map","import AbortablePromiseCache from '@gmod/abortable-promise-cache';\nimport LRU from 'quick-lru';\nimport { Buffer } from 'buffer';\nimport { RemoteFile, LocalFile } from 'generic-filehandle';\nimport { unzip, unzipChunkSlice } from '@gmod/bgzf-filehandle';\nimport { checkAbortSignal } from './util';\nimport TBI from './tbi';\nimport CSI from './csi';\nfunction isASCII(str) {\n    // eslint-disable-next-line no-control-regex\n    return /^[\\u0000-\\u007F]*$/.test(str);\n}\nconst decoder = typeof TextDecoder !== 'undefined' ? new TextDecoder('utf8') : undefined;\nexport default class TabixIndexedFile {\n    /**\n     * @param {object} args\n     *\n     * @param {string} [args.path]\n     *\n     * @param {filehandle} [args.filehandle]\n     *\n     * @param {string} [args.tbiPath]\n     *\n     * @param {filehandle} [args.tbiFilehandle]\n     *\n     * @param {string} [args.csiPath]\n     *\n     * @param {filehandle} [args.csiFilehandle]\n     *\n     * @param {url} [args.url]\n     *\n     * @param {csiUrl} [args.csiUrl]\n     *\n     * @param {tbiUrl} [args.tbiUrl]\n     *\n     * @param {function} [args.renameRefSeqs] optional function with sig `string\n     * => string` to transform reference sequence names for the purpose of\n     * indexing and querying. note that the data that is returned is not altered,\n     * just the names of the reference sequences that are used for querying.\n     */\n    constructor({ path, filehandle, url, tbiPath, tbiUrl, tbiFilehandle, csiPath, csiUrl, csiFilehandle, renameRefSeqs = n => n, chunkCacheSize = 5 * 2 ** 20, }) {\n        if (filehandle) {\n            this.filehandle = filehandle;\n        }\n        else if (path) {\n            this.filehandle = new LocalFile(path);\n        }\n        else if (url) {\n            this.filehandle = new RemoteFile(url);\n        }\n        else {\n            throw new TypeError('must provide either filehandle or path');\n        }\n        if (tbiFilehandle) {\n            this.index = new TBI({\n                filehandle: tbiFilehandle,\n                renameRefSeqs,\n            });\n        }\n        else if (csiFilehandle) {\n            this.index = new CSI({\n                filehandle: csiFilehandle,\n                renameRefSeqs,\n            });\n        }\n        else if (tbiPath) {\n            this.index = new TBI({\n                filehandle: new LocalFile(tbiPath),\n                renameRefSeqs,\n            });\n        }\n        else if (csiPath) {\n            this.index = new CSI({\n                filehandle: new LocalFile(csiPath),\n                renameRefSeqs,\n            });\n        }\n        else if (path) {\n            this.index = new TBI({\n                filehandle: new LocalFile(`${path}.tbi`),\n                renameRefSeqs,\n            });\n        }\n        else if (csiUrl) {\n            this.index = new CSI({\n                filehandle: new RemoteFile(csiUrl),\n            });\n        }\n        else if (tbiUrl) {\n            this.index = new TBI({\n                filehandle: new RemoteFile(tbiUrl),\n            });\n        }\n        else if (url) {\n            this.index = new TBI({\n                filehandle: new RemoteFile(`${url}.tbi`),\n            });\n        }\n        else {\n            throw new TypeError('must provide one of tbiFilehandle, tbiPath, csiFilehandle, csiPath, tbiUrl, csiUrl');\n        }\n        this.renameRefSeq = renameRefSeqs;\n        this.chunkCache = new AbortablePromiseCache({\n            cache: new LRU({ maxSize: Math.floor(chunkCacheSize / (1 << 16)) }),\n            fill: (args, signal) => this.readChunk(args, { signal }),\n        });\n    }\n    /**\n     * @param refName name of the reference sequence\n     *\n     * @param start start of the region (in 0-based half-open coordinates)\n     *\n     * @param end end of the region (in 0-based half-open coordinates)\n     *\n     * @param opts callback called for each line in the region. can also pass a\n     * object param containing obj.lineCallback, obj.signal, etc\n     *\n     * @returns promise that is resolved when the whole read is finished,\n     * rejected on error\n     */\n    async getLines(refName, s, e, opts) {\n        var _a, _b;\n        let signal;\n        let options = {};\n        let callback;\n        if (typeof opts === 'function') {\n            callback = opts;\n        }\n        else {\n            options = opts;\n            callback = opts.lineCallback;\n            signal = opts.signal;\n        }\n        const metadata = await this.index.getMetadata(options);\n        checkAbortSignal(signal);\n        const start = s !== null && s !== void 0 ? s : 0;\n        const end = e !== null && e !== void 0 ? e : metadata.maxRefLength;\n        if (!(start <= end)) {\n            throw new TypeError('invalid start and end coordinates. start must be less than or equal to end');\n        }\n        if (start === end) {\n            return;\n        }\n        const chunks = await this.index.blocksForRange(refName, start, end, options);\n        checkAbortSignal(signal);\n        // now go through each chunk and parse and filter the lines out of it\n        for (const c of chunks) {\n            const { buffer, cpositions, dpositions } = await this.chunkCache.get(c.toString(), c, signal);\n            checkAbortSignal(signal);\n            let blockStart = 0;\n            let pos = 0;\n            const str = (_a = decoder === null || decoder === void 0 ? void 0 : decoder.decode(buffer)) !== null && _a !== void 0 ? _a : buffer.toString();\n            // fast path, Buffer is just ASCII chars and not gigantor, can be\n            // converted to string and processed directly. if it is not ASCII or\n            // gigantic (chrome max str len is 512Mb), we have to decode line by line\n            const strIsASCII = buffer.length < 500000000 && isASCII(str);\n            while (blockStart < str.length) {\n                let line;\n                let n;\n                if (strIsASCII) {\n                    n = str.indexOf('\\n', blockStart);\n                    if (n === -1) {\n                        break;\n                    }\n                    line = str.slice(blockStart, n);\n                }\n                else {\n                    n = buffer.indexOf('\\n', blockStart);\n                    if (n === -1) {\n                        break;\n                    }\n                    const b = buffer.slice(blockStart, n);\n                    line = (_b = decoder === null || decoder === void 0 ? void 0 : decoder.decode(b)) !== null && _b !== void 0 ? _b : b.toString();\n                }\n                // eslint-disable-next-line @typescript-eslint/no-unnecessary-condition\n                if (dpositions) {\n                    while (blockStart + c.minv.dataPosition >= dpositions[pos++]) { }\n                    pos--;\n                }\n                // filter the line for whether it is within the requested range\n                const { startCoordinate, overlaps } = this.checkLine(metadata, refName, start, end, line);\n                if (overlaps) {\n                    callback(line, \n                    // cpositions[pos] refers to actual file offset of a bgzip block\n                    // boundaries\n                    //\n                    // we multiply by (1 <<8) in order to make sure each block has a\n                    // \"unique\" address space so that data in that block could never\n                    // overlap\n                    //\n                    // then the blockStart-dpositions is an uncompressed file offset\n                    // from that bgzip block boundary, and since the cpositions are\n                    // multiplied by (1 << 8) these uncompressed offsets get a unique\n                    // space\n                    cpositions[pos] * (1 << 8) +\n                        (blockStart - dpositions[pos]) +\n                        c.minv.dataPosition +\n                        1);\n                }\n                else if (startCoordinate !== undefined && startCoordinate >= end) {\n                    // the lines were overlapping the region, but now have stopped, so we\n                    // must be at the end of the relevant data and we can stop processing\n                    // data now\n                    return;\n                }\n                blockStart = n + 1;\n            }\n        }\n    }\n    async getMetadata(opts = {}) {\n        return this.index.getMetadata(opts);\n    }\n    /**\n     * get a buffer containing the \"header\" region of the file, which are the\n     * bytes up to the first non-meta line\n     */\n    async getHeaderBuffer(opts = {}) {\n        const { firstDataLine, metaChar, maxBlockSize } = await this.getMetadata(opts);\n        checkAbortSignal(opts.signal);\n        const maxFetch = ((firstDataLine === null || firstDataLine === void 0 ? void 0 : firstDataLine.blockPosition) || 0) + maxBlockSize;\n        // TODO: what if we don't have a firstDataLine, and the header\n        // actually takes up more than one block? this case is not covered here\n        const buf = await this._readRegion(0, maxFetch, opts);\n        const bytes = await unzip(buf);\n        // trim off lines after the last non-meta line\n        if (metaChar) {\n            // trim backward from the end\n            let lastNewline = -1;\n            const newlineByte = '\\n'.charCodeAt(0);\n            const metaByte = metaChar.charCodeAt(0);\n            for (let i = 0; i < bytes.length; i += 1) {\n                if (i === lastNewline + 1 && bytes[i] !== metaByte) {\n                    break;\n                }\n                if (bytes[i] === newlineByte) {\n                    lastNewline = i;\n                }\n            }\n            return bytes.subarray(0, lastNewline + 1);\n        }\n        return bytes;\n    }\n    /**\n     * get a string containing the \"header\" region of the file, is the portion up\n     * to the first non-meta line\n     *\n     * @returns {Promise} for a string\n     */\n    async getHeader(opts = {}) {\n        const bytes = await this.getHeaderBuffer(opts);\n        return bytes.toString('utf8');\n    }\n    /**\n     * get an array of reference sequence names, in the order in which they occur\n     * in the file. reference sequence renaming is not applied to these names.\n     */\n    async getReferenceSequenceNames(opts = {}) {\n        const metadata = await this.getMetadata(opts);\n        return metadata.refIdToName;\n    }\n    /**\n     * @param {object} metadata metadata object from the parsed index, containing\n     * columnNumbers, metaChar, and format\n     *\n     * @param {string} regionRefName\n     *\n     * @param {number} regionStart region start coordinate (0-based-half-open)\n     *\n     * @param {number} regionEnd region end coordinate (0-based-half-open)\n     *\n     * @param {array[string]} line\n     *\n     * @returns {object} like `{startCoordinate, overlaps}`. overlaps is boolean,\n     * true if line is a data line that overlaps the given region\n     */\n    checkLine(metadata, regionRefName, regionStart, regionEnd, line) {\n        const { columnNumbers, metaChar, coordinateType, format } = metadata;\n        // skip meta lines\n        if (metaChar && line.startsWith(metaChar)) {\n            return { overlaps: false };\n        }\n        // check ref/start/end using column metadata from index\n        let { ref, start, end } = columnNumbers;\n        if (!ref) {\n            ref = 0;\n        }\n        if (!start) {\n            start = 0;\n        }\n        if (!end) {\n            end = 0;\n        }\n        if (format === 'VCF') {\n            end = 8;\n        }\n        const maxColumn = Math.max(ref, start, end);\n        // this code is kind of complex, but it is fairly fast. basically, we want\n        // to avoid doing a split, because if the lines are really long that could\n        // lead to us allocating a bunch of extra memory, which is slow\n        let currentColumnNumber = 1; // cols are numbered starting at 1 in the index metadata\n        let currentColumnStart = 0;\n        let refSeq = '';\n        let startCoordinate = -Infinity;\n        const l = line.length;\n        for (let i = 0; i < l + 1; i++) {\n            if (line[i] === '\\t' || i === l) {\n                if (currentColumnNumber === ref) {\n                    if (this.renameRefSeq(line.slice(currentColumnStart, i)) !==\n                        regionRefName) {\n                        return {\n                            overlaps: false,\n                        };\n                    }\n                }\n                else if (currentColumnNumber === start) {\n                    startCoordinate = parseInt(line.slice(currentColumnStart, i), 10);\n                    // we convert to 0-based-half-open\n                    if (coordinateType === '1-based-closed') {\n                        startCoordinate -= 1;\n                    }\n                    if (startCoordinate >= regionEnd) {\n                        return {\n                            startCoordinate,\n                            overlaps: false,\n                        };\n                    }\n                    if (end === 0 || end === start) {\n                        // if we have no end, we assume the feature is 1 bp long\n                        if (startCoordinate + 1 <= regionStart) {\n                            return {\n                                startCoordinate,\n                                overlaps: false,\n                            };\n                        }\n                    }\n                }\n                else if (format === 'VCF' && currentColumnNumber === 4) {\n                    refSeq = line.slice(currentColumnStart, i);\n                }\n                else if (currentColumnNumber === end) {\n                    // this will never match if there is no end column\n                    const endCoordinate = format === 'VCF'\n                        ? this._getVcfEnd(startCoordinate, refSeq, line.slice(currentColumnStart, i))\n                        : Number.parseInt(line.slice(currentColumnStart, i), 10);\n                    if (endCoordinate <= regionStart) {\n                        return {\n                            overlaps: false,\n                        };\n                    }\n                }\n                currentColumnStart = i + 1;\n                currentColumnNumber += 1;\n                if (currentColumnNumber > maxColumn) {\n                    break;\n                }\n            }\n        }\n        return {\n            startCoordinate,\n            overlaps: true,\n        };\n    }\n    _getVcfEnd(startCoordinate, refSeq, info) {\n        let endCoordinate = startCoordinate + refSeq.length;\n        // ignore TRA features as they specify CHR2 and END as being on a different\n        // chromosome\n        //\n        // if CHR2 is on the same chromosome, still ignore it because there should\n        // be another pairwise feature at the end of this one\n        const isTRA = info.includes('SVTYPE=TRA');\n        if (info[0] !== '.' && !isTRA) {\n            let prevChar = ';';\n            for (let j = 0; j < info.length; j += 1) {\n                if (prevChar === ';' && info.slice(j, j + 4) === 'END=') {\n                    let valueEnd = info.indexOf(';', j);\n                    if (valueEnd === -1) {\n                        valueEnd = info.length;\n                    }\n                    endCoordinate = parseInt(info.slice(j + 4, valueEnd), 10);\n                    break;\n                }\n                prevChar = info[j];\n            }\n        }\n        else if (isTRA) {\n            return startCoordinate + 1;\n        }\n        return endCoordinate;\n    }\n    /**\n     * return the approximate number of data lines in the given reference\n     * sequence\n     *\n     * @param refSeq reference sequence name\n     *\n     * @returns number of data lines present on that reference sequence\n     */\n    async lineCount(refName, opts = {}) {\n        return this.index.lineCount(refName, opts);\n    }\n    async _readRegion(pos, size, opts = {}) {\n        const b = Buffer.alloc(size);\n        const { bytesRead, buffer } = await this.filehandle.read(b, 0, size, pos, opts);\n        return buffer.subarray(0, bytesRead);\n    }\n    /**\n     * read and uncompress the data in a chunk (composed of one or more\n     * contiguous bgzip blocks) of the file\n     */\n    async readChunk(c, opts = {}) {\n        // fetch the uncompressed data, uncompress carefully a block at a time, and\n        // stop when done\n        const data = await this._readRegion(c.minv.blockPosition, c.fetchedSize(), opts);\n        return unzipChunkSlice(data, c);\n    }\n}\n//# sourceMappingURL=tabixIndexedFile.js.map","export { default as TabixIndexedFile } from './tabixIndexedFile';\nexport { default as CSI } from './csi';\nexport { default as TBI } from './tbi';\n//# sourceMappingURL=index.js.map"],"names":[],"sourceRoot":"","ignoreList":[0,1,2,3,4,5,6,7]}